[{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"License","title":"License","text":"WORK (DEFINED ) PROVIDED TERMS CREATIVE COMMONS PUBLIC LICENSE (“CCPL” “LICENSE”). WORK PROTECTED COPYRIGHT /APPLICABLE LAW. USE WORK AUTHORIZED LICENSE COPYRIGHT LAW PROHIBITED. EXERCISING RIGHTS WORK PROVIDED , ACCEPT AGREE BOUND TERMS LICENSE. EXTENT LICENSE MAY CONSIDERED CONTRACT, LICENSOR GRANTS RIGHTS CONTAINED CONSIDERATION ACCEPTANCE TERMS CONDITIONS. Definitions “Adaptation” means work based upon Work, upon Work pre-existing works, translation, adaptation, derivative work, arrangement music alterations literary artistic work, phonogram performance includes cinematographic adaptations form Work may recast, transformed, adapted including form recognizably derived original, except work constitutes Collection considered Adaptation purpose License. avoidance doubt, Work musical work, performance phonogram, synchronization Work timed-relation moving image (“synching”) considered Adaptation purpose License. “Collection” means collection literary artistic works, encyclopedias anthologies, performances, phonograms broadcasts, works subject matter works listed Section 1(f) , , reason selection arrangement contents, constitute intellectual creations, Work included entirety unmodified form along one contributions, constituting separate independent works , together assembled collective whole. work constitutes Collection considered Adaptation (defined ) purposes License. “Distribute” means make available public original copies Work Adaptation, appropriate, sale transfer ownership. “Licensor” means individual, individuals, entity entities offer(s) Work terms License. “Original Author” means, case literary artistic work, individual, individuals, entity entities created Work individual entity can identified, publisher; addition () case performance actors, singers, musicians, dancers, persons act, sing, deliver, declaim, play , interpret otherwise perform literary artistic works expressions folklore; (ii) case phonogram producer person legal entity first fixes sounds performance sounds; , (iii) case broadcasts, organization transmits broadcast. “Work” means literary /artistic work offered terms License including without limitation production literary, scientific artistic domain, whatever may mode form expression including digital form, book, pamphlet writing; lecture, address, sermon work nature; dramatic dramatico-musical work; choreographic work entertainment dumb show; musical composition without words; cinematographic work assimilated works expressed process analogous cinematography; work drawing, painting, architecture, sculpture, engraving lithography; photographic work assimilated works expressed process analogous photography; work applied art; illustration, map, plan, sketch three-dimensional work relative geography, topography, architecture science; performance; broadcast; phonogram; compilation data extent protected copyrightable work; work performed variety circus performer extent otherwise considered literary artistic work. “” means individual entity exercising rights License previously violated terms License respect Work, received express permission Licensor exercise rights License despite previous violation. “Publicly Perform” means perform public recitations Work communicate public public recitations, means process, including wire wireless means public digital performances; make available public Works way members public may access Works place place individually chosen ; perform Work public means process communication public performances Work, including public digital performance; broadcast rebroadcast Work means including signs, sounds images. “Reproduce” means make copies Work means including without limitation sound visual recordings right fixation reproducing fixations Work, including storage protected performance phonogram digital form electronic medium. 2. Fair Dealing Rights. Nothing License intended reduce, limit, restrict uses free copyright rights arising limitations exceptions provided connection copyright protection copyright law applicable laws. License Grant. Subject terms conditions License, Licensor hereby grants worldwide, royalty-free, non-exclusive, perpetual (duration applicable copyright) license exercise rights Work stated : Reproduce Work, incorporate Work one Collections, Reproduce Work incorporated Collections; create Reproduce Adaptations provided Adaptation, including translation medium, takes reasonable steps clearly label, demarcate otherwise identify changes made original Work. example, translation marked “original work translated English Spanish,” modification indicate “original work modified.”; Distribute Publicly Perform Work including incorporated Collections; , Distribute Publicly Perform Adaptations. rights may exercised media formats whether now known hereafter devised. rights include right make modifications technically necessary exercise rights media formats. Subject Section 8(f), rights expressly granted Licensor hereby reserved, including limited rights set forth Section 4(d). Restrictions. license granted Section 3 expressly made subject limited following restrictions: may Distribute Publicly Perform Work terms License. must include copy , Uniform Resource Identifier (URI) , License every copy Work Distribute Publicly Perform. may offer impose terms Work restrict terms License ability recipient Work exercise rights granted recipient terms License. may sublicense Work. must keep intact notices refer License disclaimer warranties every copy Work Distribute Publicly Perform. Distribute Publicly Perform Work, may impose effective technological measures Work restrict ability recipient Work exercise rights granted recipient terms License. Section 4() applies Work incorporated Collection, require Collection apart Work made subject terms License. create Collection, upon notice Licensor must, extent practicable, remove Collection credit required Section 4(c), requested. create Adaptation, upon notice Licensor must, extent practicable, remove Adaptation credit required Section 4(c), requested. may exercise rights granted Section 3 manner primarily intended directed toward commercial advantage private monetary compensation. exchange Work copyrighted works means digital file-sharing otherwise shall considered intended directed toward commercial advantage private monetary compensation, provided payment monetary compensation connection exchange copyrighted works. Distribute, Publicly Perform Work Adaptations Collections, must, unless request made pursuant Section 4(), keep intact copyright notices Work provide, reasonable medium means utilizing: () name Original Author (pseudonym, applicable) supplied, /Original Author /Licensor designate another party parties (e.g., sponsor institute, publishing entity, journal) attribution (“Attribution Parties”) Licensor’s copyright notice, terms service reasonable means, name party parties; (ii) title Work supplied; (iii) extent reasonably practicable, URI, , Licensor specifies associated Work, unless URI refer copyright notice licensing information Work; , (iv) consistent Section 3(b), case Adaptation, credit identifying use Work Adaptation (e.g., “French translation Work Original Author,” “Screenplay based original Work Original Author”). credit required Section 4(c) may implemented reasonable manner; provided, however, case Adaptation Collection, minimum credit appear, credit contributing authors Adaptation Collection appears, part credits manner least prominent credits contributing authors. avoidance doubt, may use credit required Section purpose attribution manner set , exercising rights License, may implicitly explicitly assert imply connection , sponsorship endorsement Original Author, Licensor /Attribution Parties, appropriate, use Work, without separate, express prior written permission Original Author, Licensor /Attribution Parties. avoidance doubt: Non-waivable Compulsory License Schemes. jurisdictions right collect royalties statutory compulsory licensing scheme waived, Licensor reserves exclusive right collect royalties exercise rights granted License; Waivable Compulsory License Schemes. jurisdictions right collect royalties statutory compulsory licensing scheme can waived, Licensor reserves exclusive right collect royalties exercise rights granted License exercise rights purpose use otherwise noncommercial permitted Section 4(b) otherwise waives right collect royalties statutory compulsory licensing scheme; , Voluntary License Schemes. Licensor reserves right collect royalties, whether individually , event Licensor member collecting society administers voluntary licensing schemes, via society, exercise rights granted License purpose use otherwise noncommercial permitted Section 4(c). Except otherwise agreed writing Licensor may otherwise permitted applicable law, Reproduce, Distribute Publicly Perform Work either part Adaptations Collections, must distort, mutilate, modify take derogatory action relation Work prejudicial Original Author’s honor reputation. Licensor agrees jurisdictions (e.g. Japan), exercise right granted Section 3(b) License (right make Adaptations) deemed distortion, mutilation, modification derogatory action prejudicial Original Author’s honor reputation, Licensor waive assert, appropriate, Section, fullest extent permitted applicable national law, enable reasonably exercise right Section 3(b) License (right make Adaptations) otherwise. 5. Representations, Warranties Disclaimer UNLESS OTHERWISE MUTUALLY AGREED PARTIES WRITING, LICENSOR OFFERS WORK -MAKES REPRESENTATIONS WARRANTIES KIND CONCERNING WORK, EXPRESS, IMPLIED, STATUTORY OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES TITLE, MERCHANTIBILITY, FITNESS PARTICULAR PURPOSE, NONINFRINGEMENT, ABSENCE LATENT DEFECTS, ACCURACY, PRESENCE ABSENCE ERRORS, WHETHER DISCOVERABLE. JURISDICTIONS ALLOW EXCLUSION IMPLIED WARRANTIES, EXCLUSION MAY APPLY . Limitation Liability. EXCEPT EXTENT REQUIRED APPLICABLE LAW, EVENT LICENSOR LIABLE LEGAL THEORY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE EXEMPLARY DAMAGES ARISING LICENSE USE WORK, EVEN LICENSOR ADVISED POSSIBILITY DAMAGES. Termination License rights granted hereunder terminate automatically upon breach terms License. Individuals entities received Adaptations Collections License, however, licenses terminated provided individuals entities remain full compliance licenses. Sections 1, 2, 5, 6, 7, 8 survive termination License. Subject terms conditions, license granted perpetual (duration applicable copyright Work). Notwithstanding , Licensor reserves right release Work different license terms stop distributing Work time; provided, however election serve withdraw License (license , required , granted terms License), License continue full force effect unless terminated stated . 8. Miscellaneous time Distribute Publicly Perform Work Collection, Licensor offers recipient license Work terms conditions license granted License. time Distribute Publicly Perform Adaptation, Licensor offers recipient license original Work terms conditions license granted License. provision License invalid unenforceable applicable law, shall affect validity enforceability remainder terms License, without action parties agreement, provision shall reformed minimum extent necessary make provision valid enforceable. term provision License shall deemed waived breach consented unless waiver consent shall writing signed party charged waiver consent. License constitutes entire agreement parties respect Work licensed . understandings, agreements representations respect Work specified . Licensor shall bound additional provisions may appear communication . License may modified without mutual written agreement Licensor . rights granted , subject matter referenced, License drafted utilizing terminology Berne Convention Protection Literary Artistic Works (amended September 28, 1979), Rome Convention 1961, WIPO Copyright Treaty 1996, WIPO Performances Phonograms Treaty 1996 Universal Copyright Convention (revised July 24, 1971). rights subject matter take effect relevant jurisdiction License terms sought enforced according corresponding provisions implementation treaty provisions applicable national law. standard suite rights granted applicable copyright law includes additional rights granted License, additional rights deemed included License; License intended restrict license rights applicable law. Creative Commons Notice Creative Commons party License, makes warranty whatsoever connection Work. Creative Commons liable party legal theory damages whatsoever, including without limitation general, special, incidental consequential damages arising connection license. Notwithstanding foregoing two (2) sentences, Creative Commons expressly identified Licensor hereunder, shall rights obligations Licensor. Except limited purpose indicating public Work licensed CCPL, Creative Commons authorize use either party trademark “Creative Commons” related trademark logo Creative Commons without prior written consent Creative Commons. permitted use compliance Creative Commons’ -current trademark usage guidelines, may published website otherwise made available upon request time time. avoidance doubt, trademark restriction form part License. Creative Commons may contacted https://creativecommons.org/.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/MMODS-vignette.html","id":"aggregating-the-projections","dir":"Articles","previous_headings":"","what":"Aggregating the projections","title":"Aggregation and trimming on real-world COVID-19 death predictions","text":"can aggregate individual projections using either averaging approach (LOP vs. Vincent). try frameworks trimming alternatives (interior vs. exterior CDF trimming). First, define data.frame possible aggregation methods try. define averaging method (LOP vs. Vincent), method, number values trim, n_trim, whether trimming interior exterior, int_ext. , implement aggregation using methods. aggregate set cdfs, use aggregate_cdfs() function, data data.frame containing projections, id_var name column aggregated across, method \"LOP\" \"vincent\", ret_quantiles quantiles teh aggregate distribution returned, weighting_scheme n_trim define weighting scheme. can plot aggregates.","code":"# define methods to test test_trim <- expand.grid(method = c(\"LOP\", \"vincent\"),                          n_trim = c(NA,1:16),                           int_ext = c(\"interior\", \"exterior\")) test_trim$trim_type = paste(\"cdf\", test_trim$int_ext, sep = \"_\") # filter out duplicates (only trim even exterior and odd interior) test_trim = test_trim %>%    mutate(rm_flag = ifelse((int_ext == \"interior\" & n_trim %% 2 == 0) |                            (int_ext == \"exterior\" & n_trim %% 2 == 1), 1, 0)) %>%    filter(rm_flag == 0 | is.na(rm_flag)) %>%    dplyr::select(-rm_flag) # run all possible combinations ret_quant <- seq(0,1, 0.01) aggs <- foreach(i=1:nrow(test_trim)) %dopar% {   aggregate_cdfs(data = MMODS,                  id_var = \"id\",                   method = test_trim[i,\"method\"],                   ret_quantiles = ret_quant,                  weighting_scheme = ifelse(is.na(test_trim[i,\"n_trim\"]), \"equal\",                                 as.character(test_trim[i,\"trim_type\"])),                   n_trim = test_trim[i,\"n_trim\"]) } aggs <- do.call(rbind, aggs)  # create a column for each aggregation method specification aggs <- aggs %>%   mutate(id = sort(rep(1:nrow(test_trim),length(ret_quant)))) %>%   left_join(test_trim %>% mutate(id = 1:nrow(test_trim))) %>%   # change NA n_trim (mean) to 0    mutate(n_trim = ifelse(is.na(n_trim), 0, n_trim))"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/MMODS-vignette.html","id":"evaluating-the-predictions","dir":"Articles","previous_headings":"","what":"Evaluating the predictions","title":"Aggregation and trimming on real-world COVID-19 death predictions","text":"evaluate performance aggregate distribution distribution observed values. evaluating performance, let’s quick check observations. Although population size restricted 90,000-110,000 individuals, constant counties. , make sure population size isn’t confounding observed number cumulative deaths, double check correlation two.  evaluate performance, use four metrics: Log score, defined \\[log Score=log⁡(f(x))f\\] \\(N\\) number observations, \\(f()\\) predicted probability density function \\(x\\) observed value. Lower scores indicate better performance. Continuous rank probability score (CRPS), defined \\[CRPS(F,x)= \\int_{-\\infty}^\\infty (F(y)-1\\{y≥x\\})^2 dy = \\int_{-\\infty}^xF(y)^2 dy+ \\int_x^{\\infty}(F(y)-1)^2 dy\\] \\(1\\{y≥x\\}=1\\) \\(y≥x\\) 0 otherwise. Distributions lowest CRPS values calibrated (.e., observed value fall center distribution) sharp (.e., concentrated around observed value). distributions evaluated defined empirically (rather particular functional form), analytical evaluation CRPS possible. Instead, approximate CRPS using \\[CRPS(F,x)= E_F |X-x|-\\frac{1}{2} E_F |X-X'|\\] \\(E_F |X|\\) represents expectation absolute value \\(X\\) \\(X ~ F(U1)\\) \\(X' ~ F(U2)\\) \\(U1, U2 \\sim Uniform(0,1)\\). use 1,000 random uniform draws CRPS approximation. 90% interval coverage defined \\[Coverage: 90\\% = \\frac{|C_{90} |}{N} \\text{ }  \\text{ } C_{90}= {x∶F^{-1} (0.05)≤x ≤ F^{-1}(0.95)}\\] \\(x\\) observed value \\(F^{-1}\\) predicted quantile function. denote cardinality (.e., size) set, \\(\\), \\(||\\). Kullback-Leibler information criterion (KLIC) defined \\[KLIC = \\int_{-\\infty}^\\infty  [log(o(y)) - log(f(y))]o(y) dy\\] \\(o(y)\\) probability distribution observations \\(f(y)\\) predicted probability density function. approximate integral numerically using trapezoid rule. distributions defined particular observation, assume values \\(o\\) \\(f\\) close 0, specifically \\(o(y)=10^{-5}\\) \\(f(y)=10^{-5}\\). details score can found Electronic Supplementary Material 1.4 (Howerton et al., n.d.). , define function implement metrics. Now, apply function aggregate distributions. can plot outcomes performance analysis.","code":"CRPS <- function(q,v,o,size=1000, rule=2, ties=\"ordered\") {   set.seed(101) # set seed so using same uniform draws each time   x = approx(q,v,runif(size),rule=rule, ties=ties)   return(mean(abs(x$y-o[1])) -             1/2*mean(abs(x$y-approx(q,v,runif(size),rule=rule, ties=ties)$y))) }  log_score <- function(dens_x,dens_y,o,rule=2, ties=\"ordered\"){   #approx_pdf <- density(approx(q, v, runif(size))$y)   return(-log(approx(dens_x, dens_y, o[1], rule = rule, ties = ties)$y)) }  KLIC <- function(dens_x, dens_y, obs, zer = 1E-5){   obs_dist <- density(obs)   rng <- seq(0, max(obs_dist$x, dens_x),1)   obs_dist <- approx(obs_dist$x, obs_dist$y, rng, yleft = zer, yright = zer)   tst_dist <- approx(dens_x, dens_y, rng, yleft = zer, yright = zer)   return(trapz(log(obs_dist$y/tst_dist$y) * obs_dist$y))  }  cov <- function(q, v, obs, cov_level){   lwr <- v[abs(q - (1-cov_level)/2)<0.0001]   upr <- v[abs(q - (1- ((1-cov_level)/2))) < 0.0001]   return(length(which(obs < upr & obs >= lwr))/length(obs)) } # merge the aggregates and observations scores_CRPS <- crossing(aggs,MMODS_obs %>% dplyr::select(cumu_deaths) %>%                            mutate(obs_num = 1:nrow(MMODS_obs))) # set as data.table setDT(scores_CRPS) # get CRPS score scores_CRPS <- scores_CRPS[, .(CRPS = CRPS(quantile,value,cumu_deaths)),                     by=.(method, n_trim, int_ext, obs_num, cumu_deaths)]  # get log score n_samp = 10000 pdfs <- setDT(aggs)[, .(samp = approx(quantile, value, runif(n_samp))$y),               by = .(method, n_trim, int_ext)] pdfs <- pdfs[, .(dens_x = density(samp)$x,                   dens_y = density(samp)$y),               by = .(method, n_trim, int_ext)] scores_log <- crossing(pdfs, MMODS_obs %>% dplyr::select(cumu_deaths) %>% mutate(obs_num = 1:nrow(MMODS_obs))) scores_log <- setDT(scores_log)[, .(logS = log_score(dens_x, dens_y, cumu_deaths)),                     by = .(method, n_trim, int_ext, obs_num, cumu_deaths)]  # kullback-leibler scores_klic <- pdfs[, .(klic = KLIC(dens_x, dens_y, MMODS_obs$cumu_deaths)),                     by = .(method, n_trim, int_ext)]    # coverage scores_cov <- aggs[, .(cov90 = cov(quantile, value, MMODS_obs$cumu_deaths, 0.9)),                     by = .(method, n_trim, int_ext)]  # create one df for all scores scores <- merge(scores_CRPS, scores_log) scores <- scores[, .(avg_crps = mean(CRPS),                       avg_logS = mean(logS)),                   by = .(method, n_trim, int_ext)] scores <- merge(scores, scores_klic) scores <- merge(scores, scores_cov)"},{"path":[]},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/SIRS-vignette.html","id":"sirs-model","dir":"Articles","previous_headings":"","what":"SIRS Model","title":"Aggregating predictions from an SIRS model","text":"use SIRS (susceptible-infected-recovered-susceptible) model implemented stochastically chain-binomial framework (Bailey 1957). individuals population classified one three compartments (S,, R), define many individuals transition states time step: update number individuals compartment accordingly: \\[ \\begin{aligned} S(t+1) &= S(t) - \\delta_{S \\rightarrow }(t) + \\delta_{R \\rightarrow S}(t) \\\\ (t+1) &= (t) + \\delta_{S \\rightarrow }(t) - \\delta_{\\rightarrow R}(t)\\\\ R(t+1) &= R(t) + \\delta_{\\rightarrow R}(t) - \\delta_{R \\rightarrow S}(t)\\\\ \\end{aligned} \\] implement process two functions,run_cb_sir() cb_sir(), define third function collect information interested simulation, return_obj().","code":"# implement chain binomial over time  run_cb_sir = function(n_times, sims, IC, beta, gamma, rho){   class = c(\"S\", \"I\", \"R\", \"C\")   ret = array(integer(),c(sims,length(class),n_times),dimnames = list(NULL, class, NULL))   # ret dimensions - 1: sims, 2: classes, 3: time   ret[,,1] = matrix(rep(IC,sims), nrow = sims, byrow = TRUE )   # repeate cb_sir() over time   for(ts in 2:n_times){     ret[,,ts] <- cb_sir(sims,delta.t =1,                          S = ret[,\"S\",ts-1],                           I = ret[,\"I\",ts-1],                         R = ret[,\"R\",ts-1],                         C = ret[,\"C\",ts-1],                         beta=beta,gamma=gamma,rho=rho)   }   # calculate metrics of interest based on simulations   return(list(timeseries = ret,                metrics = return_obj(ret))) }  # implement chain binomial for single time step cb_sir <- function(S, I, R, C, beta, gamma, rho,                     sims, delta.t){   N <- S+I+R   # draw number of individuals transitioning between states   dN_SI <- rbinom(n=sims,size=S,prob=1-exp(-beta*I/N*delta.t))   dN_IR <- rbinom(n=sims,size=I,prob=1-exp(-gamma*delta.t))   dN_RS <- rbinom(n=sims, size=R, prob=1-exp(-rho*delta.t))   # update states   S <- S - dN_SI + dN_RS   I <- I + dN_SI - dN_IR   R <- R + dN_IR - dN_RS   C <- C + dN_SI   return(cbind(S, I, R, C)) }  # calculate metrics of interest (cumulative & peak cases) return_obj = function(out){   # select only C compartment (cumulative cases over time)   out <- reshape2::melt(out[,\"C\",], c(\"sim\", \"time\"))   setDT(out)   # cumulative cases = # cases in C at final time   cum_cases <- out[time == max(time),]   cum_cases[,time :=NULL]   setnames(cum_cases, \"value\", \"cum_cases\")   # peak cases = max(diff between C at t and t-1)   peak_cases <- out[, .(value = c(0,diff(value))), by = .(sim)]   peak_cases <- peak_cases[, .(peak_cases = max(value)), by = .(sim)]   return(left_join(peak_cases, cum_cases)) }"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/SIRS-vignette.html","id":"defining-multiple-models","dir":"Articles","previous_headings":"","what":"Defining “multiple models”","title":"Aggregating predictions from an SIRS model","text":"vary parameters represent four distinct models (\\(\\), \\(B\\), \\(C\\), \\(D\\)) varying assumptions transmission process. two types uncertainty models encapsulate: parametric uncertainty transmission rate, \\(\\beta\\). assume four models let \\(\\beta \\sim N(\\mu_\\beta, 0.2)\\) \\(\\mu_\\beta\\) varies across models, \\(\\mu_{\\beta,} = 1.2\\), \\(\\mu_{\\beta,B} = 1.4\\), \\(\\mu_{\\beta,C} = 1.6\\), \\(\\mu_{\\beta,D} = 1.8\\). structural uncertainty represented: models assume waning, .e., \\(\\rho_{,B,C,D} = 0\\) structural uncertainty represented models: two models assume waning two assume waning, .e., \\(\\rho_{,C} = 0\\) \\(\\rho_{B,D} = 1/26\\) structural uncertainty represented within models: models incorporate waning possibilities projections equal probability, .e., \\(\\rho_{,B,C,D} = [1/26,0]\\) implement cases, create data.frame individual model parameter sets consider. distributions parameters model:","code":"# define the number of simulations per model n_samples <- 10000  # define mean and sd for transmission rate (beta) for each model m1_beta <- 1.2; m2_beta <- 1.4; m3_beta <- 1.6; m4_beta <- 1.8 sig_beta <- 0.2  # define unique beta for each simulation beta <- rnorm(n_samples*4,                c(rep(m1_beta, n_samples),                  rep(m2_beta, n_samples),                 rep(m3_beta, n_samples),                 rep(m4_beta, n_samples)),                sd = sig_beta) # verify all transmission rates are positive which(beta <= 0) #> integer(0)  # define unique recovery rate (gamma) for each simulation mu_recov_time <- 1; sig_recov_time <- 0.1 recov_time <- rnorm(n_samples*4, mu_recov_time, sig_recov_time)  # verify all recovery times are positive which(recov_time <= 0) #> integer(0)  # define unique waning rate (rho) for each simulation and structural uncertainty scenario wane_time_unlikely <- 0; wane_time_likely <- 26  # structural uncertainty scenario 1: none # no models consider structural uncertainty rho_none <- rep(wane_time_unlikely, n_samples*4)   # structural uncertainty scenario 2: between models rho_btn <- c(rep(wane_time_unlikely, n_samples), # model A: waning unlikely              rep(wane_time_likely, n_samples),   # model B: waning likely              rep(wane_time_unlikely, n_samples), # model C: waning unlikely              rep(wane_time_likely, n_samples))   # model D: waning likely                # structural uncertainty scenario 3: within models # all models assume wanings/no waning with equal probability rho_win <- rep(c(rep(wane_time_unlikely, n_samples/2), rep(wane_time_likely, n_samples/2)),4)  # create data.frame of parameters for each uncertainty scenario # scenario 1: none none <- data.frame(beta = beta, recov_time = recov_time,rho = rho_none,                     model = sort(rep(LETTERS[1:4], n_samples)),                    struc_uncert = \"none\") # scenario 2: between btn <- data.frame(beta = beta, recov_time = recov_time, rho = rho_btn,                    model = sort(rep(LETTERS[1:4], n_samples)),                   struc_uncert = \"btn\") # scenario 3: within win <- data.frame(beta = beta, recov_time = recov_time, rho = rho_win,                    model = sort(rep(LETTERS[1:4], n_samples)),                   struc_uncert = \"win\")  # combine all parameters into one data.frame all_params <- rbind(none, btn, win) # correct any negative parameters and convert to rate all_params$gamma = with(all_params, ifelse(recov_time == 0, 0, 1/recov_time)) all_params <- all_params %>% select(-recov_time) all_params$rho = with(all_params, ifelse(rho == 0, 0, 1/rho)) # add simulation number all_params$sim = 1:nrow(all_params)"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/SIRS-vignette.html","id":"individual-model-projections","dir":"Articles","previous_headings":"","what":"Individual model projections","title":"Aggregating predictions from an SIRS model","text":"simulate model using parameter sets ’ve defined. , need define length simulation (52 weeks), population size (1000 individuals), initial conditions (1.5% infected, else susceptible). plot resulting simulations, show outbreaks either fade second wave depending assumptions waning immunity (green: waning, orange: waning).  simulations fade early first wave, let’s identify many simulations occurs. , time series track cumulative peak cases. summarize output distributions (pdfs cdfs) metric. vignette, show results cumulative cases, though analyses performed peak cases well. plot distribution outcomes individual model projections across three uncertainty scenarios.","code":"# other model parameters T <- 52 # length of simulation  (in weeks) N <- 1000 # number of individuals in the population init = N*c(S = 0.985, I = 0.015, R = 0, C = 0) # initial conditions  # run the chain binomial model out <- run_cb_sir(T, nrow(all_params), init , all_params$beta, all_params$gamma, all_params$rho) # reshape out metrics out$metrics <- out$metrics %>% left_join(all_params) # reshape out time series out$timeseries <- as.data.frame(out$timeseries[,\"I\",]) %>%   mutate(id = 1:n()) %>%   reshape2::melt(\"id\") %>%   rename(time = variable) %>%   mutate(time = as.integer(gsub(\"V\",\"\",time))) inital_fadeout_thresh <- 100 kable(out$metrics %>%    group_by(model, struc_uncert) %>%   #filter() %>%   summarise(n = length(which((cum_cases < inital_fadeout_thresh))))) # convert to long format out$metrics <- reshape2::melt(out$metrics, c(\"sim\", \"beta\", \"gamma\", \"rho\", \"model\", \"struc_uncert\"))  # CDFs q <- 1:999/1000 cdfs <- setDT(out$metrics)[,.(value = quantile(value, q), quantile = q),              by = .(model, struc_uncert, variable)]  # PDFs pdfs <- setDT(out$metrics)[,.(x = density(value, adjust = 2)$x, y = density(value, adjust = 2)$y),                    by = .(model, struc_uncert, variable)]"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/SIRS-vignette.html","id":"aggregating-outcomes","dir":"Articles","previous_headings":"","what":"Aggregating outcomes","title":"Aggregating predictions from an SIRS model","text":"Next, aggregate outcomes multi-model simulation using LOP Vincent averages. plot resulting aggregate distributions.","code":"# aggregate cdfs agg_cdfs <- rbind(   aggregate_cdfs(cdfs, id_var = \"model\",                   group_by = c(\"struc_uncert\", \"variable\"),                  method = \"LOP\",                   ret_quantiles = q)[,method:=\"LOP\"],   aggregate_cdfs(cdfs, id_var = \"model\",                   group_by = c(\"struc_uncert\", \"variable\"),                  method = \"vincent\",                   ret_quantiles = q)[,method := \"Vincent\"]   )    # to get aggregate pdfs, sample from std. uniform and draw from distributions agg_pdfs <- agg_cdfs[, .(samps = approx(quantile, value, runif(100000))$y),                       by = .(struc_uncert, variable, method)] %>%   .[!is.na(samps)] %>%   .[, .(x = density(samps, adjust = 1.5)$x, y = density(samps, adjust = 1.5)$y),      by = .(struc_uncert, variable, method)]"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/SIRS-vignette.html","id":"evaluating-performance","dir":"Articles","previous_headings":"","what":"Evaluating performance","title":"Aggregating predictions from an SIRS model","text":"Next test well aggregation methods perform, given range assumptions future. assumptions future transmission rate waning immunity define data.frame true parameter values. gives following distribution observations.  use continuous rank probability score (CRPS) (Matheson Winkler 1976) assess performance aggregate distributions synthetic future observations. CRPS results can plotted LOP Vincent aggregates across three structural uncertainty scenarios.  can show LOP Vincent perform better plotting difference CRPS .  , combine score differences frequency observation occurs synthetic observations, can generate distribution relative performance. summarize distribution intervals.","code":"# define the number of synthetic future observations n_obs <- 1000 # define true transmission rate obs_beta <- c(rnorm(n_obs, m1_beta, sig_beta),               rnorm(n_obs, m2_beta, sig_beta),               rnorm(n_obs, m3_beta, sig_beta),               rnorm(n_obs, m4_beta, sig_beta),               rnorm(n_obs, mean(c(m1_beta, m2_beta, m3_beta, m4_beta)),sig_beta)) # define true recovery rate obs_recov_time <- rnorm(n_obs*5, mu_recov_time, sig_recov_time)  # define true waning obs_params <- rbind(data.frame(beta = obs_beta, obs_recov_time = obs_recov_time, rho = ifelse(wane_time_likely == 0, 0, 1/wane_time_likely),                                 true_beta = sort(rep(c(paste0(\"m\", 1:4), \"mean\"), n_obs)),                                true_rho = \"Y\"),                     data.frame(beta = obs_beta, obs_recov_time = 1/obs_recov_time, rho = ifelse(wane_time_unlikely == 0, 0, 1/wane_time_unlikely),                                 true_beta = sort(rep(c(paste0(\"m\", 1:4), \"mean\"), n_obs)),                                true_rho = \"N\")) obs_params$gamma = with(obs_params, ifelse(obs_recov_time == 0, 0, 1/obs_recov_time)) obs_params$sim = 1:nrow(obs_params)  # simulate observations given these true params obs <- run_cb_sir(T, nrow(obs_params), init, obs_params$beta, obs_params$gamma, obs_params$rho) obs <- obs$metrics obs <- obs %>% left_join(obs_params) %>% select(-obs_recov_time) obs <- reshape2::melt(obs, c(\"sim\", \"beta\", \"gamma\", \"rho\", \"true_beta\", \"true_rho\")) # define function to implement CRPS  # assume piecewise linear cdf, jumps to 0/1 after last defined quantile CRPS <- function(q,v,o){   # add o into v to calculate area under curve   v <- sort(c(v, o))   # find integral cutoff where v = o   vl <- max(which(v == o))   # assume values outside the define set of values and quantiles are 0 or 1 (jump)   if(vl == 1){     q <- c(0,q)   }   else if (vl == length(v)){     q <- c(q,1)   }   else{     # add q_o into q to calculate area under curve     q <- c(q[1:(vl-1)],NA,q[(vl):length(q)])     # interpolate quantile for obs     q_o <- q[vl-1] + ((q[vl+1] - q[vl-1])/(v[vl+1] - v[vl-1]))*(o - v[vl-1])     # add q_o into q to calculate area under curve     q[vl] <- q_o   }   # calculate area of each trapezoid using CRPS formula for each subset   areas_1 <- trapz(v[1:vl], q[1:vl]^2)   areas_2 <- trapz(v[vl:length(v)], (q[vl:length(v)]-1)^2)   # return sum of areas   return(sum(c(areas_1, areas_2))) }  # implement CRPS scores <- obs %>%   select(variable, value) %>%   unique() %>%   rename(obs = value) %>%   mutate(obs_num = 1:length(obs)) %>%   dplyr::left_join(agg_cdfs) # get CRPS score scores <- setDT(scores)[, .(CRPS = CRPS(quantile,value,obs)), #                    by=.(method, variable, struc_uncert, obs_num, obs)] scores_all <- obs %>%   left_join(scores, by = c(\"variable\", \"value\" = \"obs\")) %>%   reshape2::dcast(variable + true_beta + true_rho + struc_uncert +                      value + sim ~ method, value.var = \"CRPS\") %>%   # calculate difference to see which method has better CRPS   mutate(diff = Vincent - LOP) %>%   # summarize distribution of differences   group_by(variable, true_beta, true_rho, struc_uncert) %>%   mutate(lower = quantile(diff, 0.025),       lower_25 = quantile(diff, 0.25),        med = median(diff),       upper_75 = quantile(diff, 0.75),       upper = quantile(diff, 0.975),        # for plotting       plot_y = factor(true_beta)) %>%   mutate(plot_y = as.numeric(plot_y))"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/SIRS-vignette.html","id":"aggregating-with-an-outlier","dir":"Articles","previous_headings":"","what":"Aggregating with an outlier","title":"Aggregating predictions from an SIRS model","text":"Lastly, consider effects outlier resulting aggregate distributions. run another set model simulations \\(\\mu_\\beta = 2.4\\). approximate individual distributions, including outlier, plot distribution outcomes individual model projections across three uncertainty scenarios.  Next, aggregate predictions without trimming (note consider trimming two values, lowest highest, five models). plot resulting aggregate distributions.  Let’s see aggregates perform alternate truth scenarios.   Lastly, plot often aggregate distribution best performer.","code":"# define unique beta for each simulation outlier_mu_beta <- 2.4 outlier_beta <- rnorm(n_samples,                        outlier_mu_beta,                        sd = sig_beta ) # verify all transmission rates are positive which(beta <= 0) #> integer(0)  # define unique recovery rate (gamma) for each simulation outlier_recov_time <- rnorm(n_samples, mu_recov_time, sig_recov_time)  # verify all recovery times are positive which(recov_time <= 0) #> integer(0)   # create data.frame of parameters for each uncertainty scenario # scenario 1: none none <- data.frame(beta = outlier_beta, recov_time = outlier_recov_time,                    rho = wane_time_unlikely,                     model = \"E\",                    struc_uncert = \"none\") # scenario 2: between btn <- data.frame(beta = outlier_beta, recov_time = outlier_recov_time,                    rho = wane_time_unlikely,                    model = \"E\",                   struc_uncert = \"btn\") # scenario 3: within win <- data.frame(beta = outlier_beta, recov_time = outlier_recov_time,                    rho = c(rep(wane_time_unlikely, n_samples/2), rep(wane_time_likely, n_samples/2)),                    model = \"E\",                   struc_uncert = \"win\")  # combine all parameters into one data.frame outlier_params <- rbind(none, btn, win) # correct any negative parameters and convert to rate outlier_params$gamma = with(outlier_params, ifelse(recov_time == 0, 0, 1/recov_time)) outlier_params <- outlier_params %>% select(-recov_time) outlier_params$rho = with(outlier_params, ifelse(rho == 0, 0, 1/rho)) # add simulation number outlier_params$sim = max(all_params$sim) + 1:nrow(outlier_params) # run the chain binomial model outlier_out <- run_cb_sir(T, nrow(outlier_params), init , outlier_params$beta, outlier_params$gamma, outlier_params$rho) # reshape outlier_out metrics outlier_out$metrics <- outlier_out$metrics %>%    mutate(sim = sim + max(all_params$sim)) %>%   left_join(outlier_params) # reshape outlier_out time series outlier_out$timeseries <- as.data.frame(outlier_out$timeseries[,\"I\",]) %>%   mutate(id = 1:n() + max(all_params$sim)) %>%   reshape2::melt(\"id\") %>%   rename(time = variable) %>%   mutate(time = as.integer(gsub(\"V\",\"\",time))) # convert to long format outlier_out$metrics <- reshape2::melt(outlier_out$metrics, c(\"sim\", \"beta\", \"gamma\", \"rho\", \"model\", \"struc_uncert\"))  # CDFs q <- 1:999/1000 outlier_cdfs <- setDT(outlier_out$metrics)[,.(value = quantile(value, q), quantile = q),              by = .(model, struc_uncert, variable)]  # PDFs outlier_pdfs <- setDT(outlier_out$metrics)[,.(x = density(value, adjust = 2)$x, y = density(value, adjust = 2)$y),                    by = .(model, struc_uncert, variable)] # aggregate cdfs agg_cdfs_outlier <- rbind(   aggregate_cdfs(bind_rows(cdfs, outlier_cdfs), id_var = \"model\",                   group_by = c(\"struc_uncert\", \"variable\"),                  method = \"LOP\",                   ret_quantiles = q) %>%     .[, \":=\" (method=\"LOP\",                ntrim = 0)],   aggregate_cdfs(bind_rows(cdfs, outlier_cdfs), id_var = \"model\",                   group_by = c(\"struc_uncert\", \"variable\"),                  method = \"vincent\",                   ret_quantiles = q) %>%     .[, \":=\" (method=\"Vincent\",                ntrim = 0)],    # add trimmed aggregates   aggregate_cdfs(bind_rows(cdfs, outlier_cdfs), id_var = \"model\",                   group_by = c(\"struc_uncert\", \"variable\"),                  method = \"LOP\",                   ret_quantiles = q,                   weighting_scheme = \"CDF_exterior\",                   n_trim = 2) %>%     .[, \":=\" (method=\"LOP\",                ntrim = 2)],   aggregate_cdfs(bind_rows(cdfs, outlier_cdfs), id_var = \"model\",                   group_by = c(\"struc_uncert\", \"variable\"),                  method = \"vincent\",                   ret_quantiles = q,                   weighting_scheme = \"CDF_exterior\",                   n_trim = 2) %>%     .[, \":=\" (method=\"Vincent\",                ntrim = 2)]   )    # to get aggregate pdfs, sample from std. uniform and draw from distributions agg_pdfs_outlier <- agg_cdfs_outlier[, .(samps = approx(quantile, value, runif(100000))$y),                       by = .(struc_uncert, variable, method, ntrim)] %>%   .[!is.na(samps)] %>%   .[, .(x = density(samps, adjust = 1.5)$x, y = density(samps, adjust = 1.5)$y),      by = .(struc_uncert, variable, method, ntrim)] # implement CRPS scores_outlier <- obs %>%   select(variable, value) %>%   unique() %>%   rename(obs = value) %>%   mutate(obs_num = 1:length(obs)) %>%   dplyr::left_join(agg_cdfs_outlier) # get CRPS score scores_outlier <- setDT(scores_outlier)[, .(CRPS = CRPS(quantile,value,obs)), #                    by=.(method, ntrim, variable, struc_uncert, obs_num, obs)]"},{"path":[]},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/intro-vignette.html","id":"unweighted-aggregation","dir":"Articles","previous_headings":"","what":"Unweighted aggregation","title":"Introduction to CombineDistributions","text":"aggregating probabilistic predictions, can perform operations probabilities quantiles. , focus averages (.e., probability averaging Linear Opinion Pool, quantile averaging Vincent average). consider set probabilistic predictions, represented cumulative distribution function (CDF), \\(F_i(x)\\) \\(x\\) possible future outcome \\(F_i(x)\\) probability outcome less equal \\(x\\). , define quantile function, \\(F_i^{-1}(\\theta)\\) quantile \\(\\theta\\). aggregate using LOP method, compute arithmetic mean cumulative probabilities \\(x\\) value, \\[ F_{LOP}(x) = \\sum_{= 1}^{N}w_iF_i(x) \\] \\(N\\) models weights \\(w_i\\). Conversely, Vincent average, calculate arithmetic mean values, \\(x\\), quantile \\(\\theta\\), \\[ F_{V}(\\theta) = \\sum_{= 1}^{N}w_iF_i^{-1}(\\theta) \\] also \\(N\\) models weights \\(w_i\\).","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/intro-vignette.html","id":"a-simple-example","dir":"Articles","previous_headings":"Unweighted aggregation","what":"A simple example","title":"Introduction to CombineDistributions","text":", show implement two methods using CombineDistributions simple example using normal distributions. First, let’s load packages need. Specifically, example aggregate two normal distributions, \\(N(\\mu = 100, \\sigma = 10)\\) \\(N(\\mu = 120, \\sigma = 5)\\). Let’s define two distributions particular values, x. can aggregate distributions using aggregate_cdfs() function. function requires distributions included single data.frame object columns quantile value (define CDF) column distinguish predictions. let’s put predictions single data.frame add id column. Now, let’s aggregate using LOP. setting method = \"LOP\" function call. also define quantiles like aggregate CDF returned. can also aggregate using Vincent average. Now, let’s plot individual (black) aggregate predictions cumulative distribution functions (CDFs).  Next ’ll plot probability density functions (PDFs), \\(f_i(x)\\) highlight difference two approaches. predictions defined CDFs, use tricks obtain PDFs. LOP operation defined (operating CDFs) equivalent operating PDFs, \\(f_{LOP}(x) = \\sum_{= 1}^Nw_if_i(x)\\). Vincent average distributions location-scale family (.e., defined location scale paramter) also location-scale family, parameters aggregate average individual distributions (Thomas Ross 1980). property applies case, aggregating normal distributions. , Vincent average aggregate normal distribution \\(\\mu_V = mean(\\mu_1, \\mu_2)\\) \\(\\sigma_v = mean(\\sigma_1, \\sigma_2)\\). Let’s implement operations plot PDFs. First, define equivalent individual PDFs. Now, let’s implement rule find PDF LOP Vincent average aggregates. Plotting results demonstrates difference two methods: LOP treats individual predictions alternate possibilities, retains uncertainty captured individual predictions, whereas Vincent average treats individual predictions noisy samples averages away uncertainty captured predictions.  cases rules apply, can obtain aggregate PDFs sampling aggregate CDF. See SIRS vignette example.","code":"# for aggregation library(CombineDistributions)  # for data manipulation library(dplyr)  # for plotting library(ggplot2) library(cowplot) # set range of x values over which to define cdfs x <- 80:140  # set CDFs mean1 = 100 sig1 = 10 pred1 <- data.frame(value = x,                       quantile = pnorm(x, mean1, sig1)) mean2 = 120 sig2 = 5 pred2 <- data.frame(value = x,                       quantile = pnorm(x, mean2, sig2)) preds <- bind_rows(pred1 %>%                       mutate(id = \"A\"),                     pred2 %>%                       mutate(id = \"B\")) rq <- seq(0.01,0.99, 0.01)  LOP <- aggregate_cdfs(data = preds, # predictions                id_var = \"id\",       # prediction identifier                method = \"LOP\",      # aggregation method                ret_quantiles = rq   # quantiles to return                ) vin <- aggregate_cdfs(data = preds, # predictions                id_var = \"id\",       # prediction identifier                method = \"vincent\",  # aggregation method                ret_quantiles = rq   # quantiles to return                ) # set PDFs mean1 = 100 sig1 = 10 pred1_pdf <- data.frame(value = x,                          prob = dnorm(x, mean1, sig1)) mean2 = 120 sig2 = 5 pred2_pdf <- data.frame(value = x,                          prob = dnorm(x, mean2, sig2))  # combine into single data.frame preds_pdf <- bind_rows(pred1_pdf %>%                           mutate(id = \"A\"),                         pred2_pdf %>%                           mutate(id = \"B\")) LOP_pdf <- preds_pdf %>%   group_by(value) %>%    summarize(prob = mean(prob))  vin_pdf <- data.frame(value = x,                        prob = dnorm(x, mean(c(mean1, mean2)), c(mean(sig1, sig2))))"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/intro-vignette.html","id":"aggregating-across-multiple-groups","dir":"Articles","previous_headings":"Unweighted aggregation","what":"Aggregating across multiple groups","title":"Introduction to CombineDistributions","text":"Let’s say also death predictions model. illustrate, add simple death predictions existing data.frame case predictions. aggregate sets predictions time, use group_by argument, provide vector column names provide additional strata/groupings. , one aggregate distribution returned unique combination group_by elements. can plot two aggregates see full distribution.","code":"# take the predictions from the first example and specify they are   # predictions of incident cases preds <- preds %>%    mutate(target = \"incident cases\")  # define predictions of incident deaths for A and B and add to preds data.frame preds <- preds %>%    bind_rows(     data.frame(value = rep(seq(0,20, 0.1), times = 2),                 quantile = c(pnorm(seq(0,20, 0.1), 10, 2),                              pnorm(seq(0,20, 0.1), 12, 3)),                 id = sort(rep(LETTERS[1:2],length(seq(0,20, 0.1)))),                 target = \"incident deaths\")   ) # aggregate case and death targets separately aggs <- aggregate_cdfs(data = preds,            # predictions                        id_var = \"id\",           # prediction identifier                        group_by = c(\"target\"),  # column names that specify groups                        method = \"LOP\",          # aggregation method                        ret_quantiles = rq       # quantiles to return )   # aggs object has both incident cases and deaths head(aggs %>% arrange(quantile)) #>             target quantile     value #> 1:  incident cases     0.01 80.000000 #> 2: incident deaths     0.01  5.204324 #> 3:  incident cases     0.02 82.471293 #> 4: incident deaths     0.02  5.869789 #> 5:  incident cases     0.03 84.433107 #> 6: incident deaths     0.03  6.284290"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/intro-vignette.html","id":"implementation-details","dir":"Articles","previous_headings":"","what":"Implementation details","title":"Introduction to CombineDistributions","text":"CDF checks: CombineDistributions package implements two checks distributions aggregated. monotonic CDFs: set increasing quantiles, \\(q_i\\), check values also increasing monotonically, .e., \\(F(q_i) \\leq F(q_{+1})\\). NA values: check quantile value NA. CDF fails either check, excluded aggregation warning message displayed. Defining CDF: package assumes CDFs individual prediction defined set value-quantile pairs. Therefore, method needed interpolate quantiles values defined pairs , cases, extrapolate beyond set value-quantile pairs. package, use linear interpolation (approx()). limitations choice significant CDF defined fewer value-quantile pairs. assume CDF jumps 0 1 outside smallest largest value-quantile pairs. attempt estimate tail probabilities, recommended tail probabilities individual distribution sufficiently well defined (espeically probabilities interest aggregate). example set value-quantile pairs corresponding piecewise-linear approximation jumps aggregated CombineDistributions.","code":""},{"path":[]},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/tidy-tuesday-example-analysis.html","id":"daily-high-temperature-forecasts-for-baltimore-md","dir":"Articles","previous_headings":"","what":"Daily High Temperature Forecasts for Baltimore, MD","title":"Example Analysis","text":"data data dictionary available .","code":"library(tidytuesdayR) # contains function to load data; alternative is to download from github URL (see below) library(usethis) library(tidyr) library(dplyr) library(purrr) library(CombineDistributions)  library(ggplot2) library(cowplot) # test if a directory named \"data\" exists locally, if not create it if (!dir.exists(\"../data\")){   dir.create(\"../data\") }  # read the data; download locally if not already downloaded if (!file.exists(\"../data/tuesdata.rda\")){   tuesdata <- tidytuesdayR::tt_load('2022-12-20') # can also use tidytuesdayR::tt_load(2022, week = 51)   usethis::use_data(tuesdata, overwrite = TRUE)      # Alternatively, read in the data manually   # weather_forecasts <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/weather_forecasts.csv')   # cities <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/cities.csv')   # outlook_meanings <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/outlook_meanings.csv') } else data(tuesdata) #>  #>  Downloading file 1 of 3: `weather_forecasts.csv` #>  Downloading file 2 of 3: `cities.csv` #>  Downloading file 3 of 3: `outlook_meanings.csv`  # get relevant data frame(s) weather_forecasts <- tuesdata$weather_forecasts  # get Baltimore Dec 1-31, 2021 data bmore_forecasts <- weather_forecasts %>%   filter(state == \"MD\", city == \"BALTIMORE\", date >= as.Date(\"2021-12-01\"), date <= as.Date(\"2021-12-31\")) %>%   drop_na(forecast_temp)  # get high temperatures highs <- bmore_forecasts %>%   filter(high_or_low == \"high\") %>%   select(-c(city, state, high_or_low, observed_precip, forecast_outlook, possible_error)) %>%   pivot_longer(cols = c(forecast_temp, observed_temp), names_to = \"type\", values_to = \"temp\")  # split data into lists highs_by_hour <- highs %>%   group_by(forecast_hours_before) %>%   group_split() highs_by_date <- highs %>%   group_by(date) %>%   group_split() names(highs_by_date) <- unique(highs$date)"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/tidy-tuesday-example-analysis.html","id":"exploratory-plot","dir":"Articles","previous_headings":"","what":"Exploratory Plot","title":"Example Analysis","text":"","code":"ggplot(highs) +   geom_line(aes(x = date, y = temp, linetype = type)) +   facet_wrap(vars(forecast_hours_before), labeller = as_labeller(c(`12` = \"12 hours ahead\",                                                                    `24` = \"24 hours ahead\",                                                                    `36` = \"36 hours ahead\",                                                                    `48` = \"48 hours ahead\"))) +   theme_bw() +   xlab(\"Date (in December 2021)\") +   ylab(\"Temperature (F)\") +   scale_linetype_discrete(name = \"\", labels = c(\"Forecast\", \"Observed\")) +   ggtitle(\"Forecasted vs. Observed Daily High Temperature in Baltimore, MD\\nDecember 1-31, 2021\")"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/tidy-tuesday-example-analysis.html","id":"simulate-distributions-for-the-weather-forecasts","dir":"Articles","previous_headings":"","what":"Simulate Distributions for the Weather Forecasts","title":"Example Analysis","text":"CombineDistributions package designed aggregate multiple probabilistic models single probabilistic model. example, treat 12-hour, 24-hour, 36-hour, 48-hour-ahead forecasts four separate models estimand (daily high temperature). Unfortunately, dataset provides point estimates , measures uncertainty (confidence interval, better yet using package, probability density). Therefore, generate normal distribution model, centered National Weather Service’s point estimate using empirical standard deviation model since expect -ahead forecasts greater uncertainty.","code":"quantiles_to_be_specified <- seq(0.01, 0.99, 0.01)  high_forecasts_by_hour <- map(.x = highs_by_hour, .f = ~filter(.x, type == \"forecast_temp\")) high_forecasts_by_date <- map(.x = highs_by_date, .f = ~filter(.x, type == \"forecast_temp\")) high_forecasts_by_date[[15]] <- NULL # only 12, 24, and 36 hour ahead forecasts on Dec 15 high_forecasts_by_date[[14]] <- NULL # only 12, 36, and 48 hour ahead forecasts on Dec 14  sd_vec <- map_dbl(.x = high_forecasts_by_hour, .f = ~sd(.x$temp))  high_distributions_by_date <- lapply(X = high_forecasts_by_date,                                      FUN = function(date_tibble) map2(.x = date_tibble %>% arrange(forecast_hours_before) %>% select(temp),                                                                       .y = sd_vec,                                                                       .f = ~qnorm(quantiles_to_be_specified, mean = .x, sd = .y))) %>%   lapply(function(ls) data.frame(value = unlist(ls),                                  quantile = rep(quantiles_to_be_specified, 4),                                  id = rep(c(\"12\", \"24\", \"36\", \"48\"), each = length(quantiles_to_be_specified))))  high_distributions_unlisted <- 1:length(high_distributions_by_date) %>%   lapply(function(i) mutate(high_distributions_by_date[[i]], date = names(high_distributions_by_date)[i])) %>%   bind_rows"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/tidy-tuesday-example-analysis.html","id":"analyze-forecast-models-using-lop-and-vincent","dir":"Articles","previous_headings":"","what":"Analyze Forecast Models using LOP() and vincent()","title":"Example Analysis","text":"can now aggregate 12-hour, 24-hour, 36-hour, 48-hour-ahead forecasts day December 2021, first using linear opinion pool (LOP) averaging, using Vincent averaging. illustration purposes, show result December 31, 2021.","code":"dec31_high_distributions <- high_distributions_by_date[[29]] %>%   arrange(quantile)  dec31_lop <- LOP(quantile = dec31_high_distributions$quantile,                  value = dec31_high_distributions$value,                  id = as.character(dec31_high_distributions$id),                  ret_quantiles = quantiles_to_be_specified,                  ret_values = NA)  dec31_vincent <- vincent(quantile = dec31_high_distributions$quantile,                          value = dec31_high_distributions$value,                          id = as.character(dec31_high_distributions$id),                          ret_quantiles = quantiles_to_be_specified,                          ret_values = NA)  # get observed high temp and point estimates for each model dec31_obs <- highs %>%   filter(date == as.Date(\"2021-12-31\"), type == \"observed_temp\") %>%   arrange(forecast_hours_before) dec31_pred <- highs %>%   filter(date == as.Date(\"2021-12-31\"), type == \"forecast_temp\") %>%   arrange(forecast_hours_before) # set PDFs x_min <- round(min(dec31_high_distributions$value)) x_max <- round(max(dec31_high_distributions$value)) x <- x_min:x_max  pred1_pdf <- data.frame(value = x,                          prob = dnorm(x, dec31_pred$temp[1], sd_vec[1])) pred2_pdf <- data.frame(value = x,                          prob = dnorm(x, dec31_pred$temp[2], sd_vec[2])) pred3_pdf <- data.frame(value = x,                          prob = dnorm(x, dec31_pred$temp[3], sd_vec[3])) pred4_pdf <- data.frame(value = x,                          prob = dnorm(x, dec31_pred$temp[4], sd_vec[4]))  # combine into single data.frame preds_pdf <- bind_rows(pred1_pdf %>%                           mutate(id = \"12\"),                         pred2_pdf %>%                           mutate(id = \"24\"),                        pred3_pdf %>%                           mutate(id = \"36\"),                         pred4_pdf %>%                           mutate(id = \"48\")) LOP_pdf <- preds_pdf %>%   group_by(value) %>%    summarize(prob = mean(prob))  vin_pdf <- data.frame(value = x,                        prob = dnorm(x, mean(dec31_pred$temp), c(mean(sd_vec)))) both_pdf <- ggplot(data = preds_pdf,                    aes(x = value, y = prob)) +   geom_line(aes(group = id), color = \"black\", size = 0.5) +   geom_line(data = LOP_pdf, aes(color = \"Linear Opinion Pool\"), size = 1) +   geom_line(data = vin_pdf, aes(color = \"Vincent average\"), size = 1) +   geom_vline(data = dec31_obs, aes(xintercept = temp), linetype = \"dashed\") +   labs(x = \"Temperature (F)\",         y = \"Probability Density Function (PDF)\",        caption = \"Predicted High Temperature for December 31, 2021\\nObserved High Temperature as Dashed Line\",        title = \"Aggregated High Temperature Forecasts for December 31, 2021\\n(Observed High Temperature as Dashed Line)\")+   scale_color_manual(values = c(\"#377EB8\", \"#ff7f00\"))+   theme_classic()+   theme(legend.position = c(0.2,0.9),          legend.title = element_blank(),         plot.caption = element_text(hjust = 0.5)) both_pdf"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/tidy-tuesday-example-analysis.html","id":"concluding-thoughts","dir":"Articles","previous_headings":"","what":"Concluding Thoughts","title":"Example Analysis","text":"illustration, can see linear opinion pool uncertainty Vincent average, seemed better since models overestimated high temperature December 31, 2021. next step, want plot LOP Vincent average models day December 1 31, 2021 time series, uncertainties expressed geom_ribbon() perhaps 95% prediction intervals.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/articles/tidy-tuesday-example-analysis.html","id":"list-of-functions-used","dir":"Articles","previous_headings":"","what":"List of Functions Used","title":"Example Analysis","text":"usethis use_data() dplyr filter() select() group_by() group_split() tidyr pivot_longer() drop_na() purrr map() map_dbl() map2() pmap() ggplot2 facet_wrap() geom_line() geom_vline()","code":""},{"path":[]},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Emily Howerton. Author, maintainer.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Howerton E (2023). CombineDistributions: Combine probabilistic predictions averaging probabilities quantiles. https://m-qin.github.io/CombineDistributions/, https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/.","code":"@Manual{,   title = {CombineDistributions: Combine probabilistic predictions by averaging probabilities or quantiles},   author = {Emily Howerton},   year = {2023},   note = {https://m-qin.github.io/CombineDistributions/, https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/}, }"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/index.html","id":"michelle-qins-additions-for-biostat-777-class","dir":"","previous_headings":"","what":"Michelle Qin’s additions for Biostat 777 class","title":"Combine probabilistic predictions by averaging probabilities or quantiles","text":"Biostat 777 class (taken fall 2023), thrilled create website R package CombineDistributions. first used CombineDistributions two half years ago worked Dr. Emily Howerton (creator package), Dr. Cecile Viboud, scientists COVID-19 Scenario Modeling Hub. task project U.S. COVID-19 cases, hospitalizations, deaths one week six months ahead. long-term infectious disease modeling required us carefully combine multiple expert modeling teams’ projections quantify combined uncertainty, motivating Dr. Howerton et al.’s methodological work area.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/index.html","id":"the-pkgdown-website","dir":"","previous_headings":"Michelle Qin’s additions for Biostat 777 class","what":"The pkgdown website","title":"Combine probabilistic predictions by averaging probabilities or quantiles","text":"Using pkgdown, created website help users use CombineDistributions package. made customizations website: set theme website “yeti”, Bootswatch theme. reordered vignettes navbar start introduction rather following alphabetical order, default. sidebar homepage footer website, credited Dr. Howerton author creator package. updated sidebar homepage reflect license Dr. Howerton’s original package uses CC--NC. DESCRIPTION file, wrote title website: “Combine probabilistic predictions averaging probabilities quantiles”. updated README package, printed homepage website well, include paper Dr. Howerton et al. published package published Zenodo. errors running devtools::check(), edited package’s scripts files. may caught everything though. trouble especially setting root directory project.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/index.html","id":"exported-functions-in-this-package","dir":"","previous_headings":"Michelle Qin’s additions for Biostat 777 class","what":"Exported functions in this package","title":"Combine probabilistic predictions by averaging probabilities or quantiles","text":"CombineDistributions exports following functions: LOP(): “Given set cumulative distribution functions (assume now: defined values), combine using probability averaging (also called linear opinion pool). method calculates (weighted) average quantiles given value.” aggregate_cdfs(), defined implementAggregation.R file: “Given data.frame containing cdfs, return single aggregate cdf using specified method.” vincent(): “Given set cumulative distribution functions (assume now: defined values), combine using quantile averaging (also called Vincent average). method calculates (weighted) average values given quantile.” basic example function LOP(), taken documentation:","code":"dat <- expand.grid(id = c(\"A\", \"B\"),                    quantile = seq(0,1,0.01)) dat$value <- ifelse(dat$id == \"A\", qnorm(dat$quantile), qnorm(dat$quantile, 0,2)) LOP(dat$quantile, dat$value, dat$id, seq(0,1,0.05), NA)"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/index.html","id":"miscellaneous-comments","dir":"","previous_headings":"","what":"Miscellaneous comment(s)","title":"Combine probabilistic predictions by averaging probabilities or quantiles","text":"original package exports data/ folder, containing files generated R scripts data-raw/ folder, exported script called data.R R folder, used vignette. Following TA’s advice R packages, deleted data/ folder R/data.R forked repository.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/index.html","id":"combinedistributions","dir":"","previous_headings":"","what":"Combine probabilistic predictions by averaging probabilities or quantiles","title":"Combine probabilistic predictions by averaging probabilities or quantiles","text":"package implements multiple methods combining probabilistic predictions, including probability quantile averaging, .e., Linear Opinion Pool (Stone 1961) Vincent average (Vincent 1912, Ratcliff 1979) respectively, well non-equal weighting, including trimming methods (Jose 2014). code licensed CC--NC (Creative Commons attribution-noncommercial) license. package can referenced citing available GitHub https://github.com/eahowerton/CombineDistributions.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Combine probabilistic predictions by averaging probabilities or quantiles","text":"can install released version CombineDistributions GitHub :","code":"remotes::install_github(\"eahowerton/CombineDistributions\")"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/index.html","id":"vignettes","dir":"","previous_headings":"","what":"Vignettes","title":"Combine probabilistic predictions by averaging probabilities or quantiles","text":"Please reference vignettes see examples functionality CombineDistributions. Introduction CombineDistributions: vignette provides overview aggregation methods available implement . Aggregating predictions SIRS mdoel: vignette illustrates simple simulation case study, simulate multi-model prediction effort compare aggregation methods controlled environment. Aggregation trimming real-world COVID-19 death predictions: vignette aggregates projections COVID-19 deaths 17 distinct epidemiological models, including varying aggregation approach weighting scheme.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Combine probabilistic predictions by averaging probabilities or quantiles","text":"Howerton, Emily, Runge, Michael C., Bogich, Tiffany L., Borchering, Rebecca K., Inamine, Hidetoshi, Lessler, Justin, Mullany, Luke C., Probert, William J. M., Smith, Claire P., Truelove, Shaun, Viboud, Cécile, Shea, Katriona. 2023. “Context-dependent representation within- -model uncertainty: aggregating probabilistic predictions infectious disease epidemiology.” J. R. Soc. Interface 20:20220659. http://doi.org/10.1098/rsif.2022.0659. Jose, Victor Richmond R., Yael Grushka-Cockayne, Kenneth C. Lichtendahl. 2014. “Trimmed Opinion Pools Crowd’s Calibration Problem.” Management Science 60 (2): 463–75. https://doi.org/10.1287/mnsc.2013.1781. Ratcliff, Roger. 1979. “Group Reaction Time Distributions Analysis Distribution Statistics.” Psychological Bulletin 86 (3): 446–61. https://doi.org/10.1037/0033-2909.86.3.446. Stone, M. 1961. “Opinion Pool.” Annals Mathematical Statistics 32 (4): 1339–42. Vincent, Stella Burnham. 1912. “Function Vibrissae Behavior White Rat.” Cambridge MA: Holt.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/LOP.html","id":null,"dir":"Reference","previous_headings":"","what":"Implement probability averaging — LOP","title":"Implement probability averaging — LOP","text":"Given set cumulative distribution functions  (assume now: defined values), combine using probability averaging (also called linear opinion pool). #TODO CITE method calculates (weighted) average quantiles given value","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/LOP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Implement probability averaging — LOP","text":"","code":"LOP(   quantile,   value,   id,   ret_quantiles,   ret_values,   weight_fn = equal_weights,   ... )"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/LOP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Implement probability averaging — LOP","text":"quantile vector containing quantiles cdfs aggregated value vector containing values cdfs aggregated id vector containing unique ids distinguish cdf aggregated ret_quantiles vector quantiles return specifying aggregate distribution ret_values vector values return specifying aggregate distribution weight_fn function? specifying weight model *FIX ... additional arguments","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/LOP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Implement probability averaging — LOP","text":"vector values corresponding ret_quantiles aggregate distribution","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/LOP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Implement probability averaging — LOP","text":"","code":"dat <- expand.grid(id = c(\"A\", \"B\"),                    quantile = seq(0,1,0.01)) dat$value <- ifelse(dat$id == \"A\", qnorm(dat$quantile), qnorm(dat$quantile, 0,2)) LOP(dat$quantile, dat$value, dat$id, seq(0,1,0.05), NA) #> # A tibble: 21 × 2 #>    quantile    value #>       <dbl>    <dbl> #>  1     0    -Inf     #>  2     0.05  NaN     #>  3     0.1    -1.90  #>  4     0.15   -1.48  #>  5     0.2    -1.17  #>  6     0.25   -0.924 #>  7     0.3    -0.710 #>  8     0.35   -0.518 #>  9     0.4    -0.339 #> 10     0.45   -0.168 #> # ℹ 11 more rows"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/MMODS.html","id":null,"dir":"Reference","previous_headings":"","what":"Projections from Multiple Models for Outbreak Decision Support (MMODS) study — MMODS","title":"Projections from Multiple Models for Outbreak Decision Support (MMODS) study — MMODS","text":"data includes projections 17 distinct models elicited first round MMODS study. projections cumulative deaths May-November 2020 US county approximately 100,000 individuals implements strict stay--home order duration projection period. Details elicitation data can found Shea et al. 2020.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/MMODS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Projections from Multiple Models for Outbreak Decision Support (MMODS) study — MMODS","text":"","code":"MMODS"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/MMODS.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Projections from Multiple Models for Outbreak Decision Support (MMODS) study — MMODS","text":"data frame 1700 rows 3 variables: id id differentiate projections distinct models quantile projection quantile 0 1 value projected cumulative number deaths quantile","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/MMODS.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Projections from Multiple Models for Outbreak Decision Support (MMODS) study — MMODS","text":"https://github.com/MMODS-org/Elicitation-1","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/MMODS_obs.html","id":null,"dir":"Reference","previous_headings":"","what":"Observations from Multiple Models for Outbreak Decision Support (MMODS) study — MMODS_obs","title":"Observations from Multiple Models for Outbreak Decision Support (MMODS) study — MMODS_obs","text":"data includes observations 84 US counties population 90,000 110,000 individuals implemneted strict stay--home orders May November 2020. observations represent cumulative deaths counties. Details elicitation data can found Shea et al. 2020.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/MMODS_obs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Observations from Multiple Models for Outbreak Decision Support (MMODS) study — MMODS_obs","text":"","code":"MMODS_obs"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/MMODS_obs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Observations from Multiple Models for Outbreak Decision Support (MMODS) study — MMODS_obs","text":"data frame 84 rows 5 variables: FIPS FIPS code county County name county State state county Population popualtion size county cumu_deaths number COVID-19 deaths recorded county November 15, 2020","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/MMODS_obs.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Observations from Multiple Models for Outbreak Decision Support (MMODS) study — MMODS_obs","text":"https://github.com/MMODS-org/Elicitation-1","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/aggregate_cdfs.html","id":null,"dir":"Reference","previous_headings":"","what":"aggregate a set of cdfs — aggregate_cdfs","title":"aggregate a set of cdfs — aggregate_cdfs","text":"Given data.frame containing cdfs , return single aggregate cdf using specified method.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/aggregate_cdfs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"aggregate a set of cdfs — aggregate_cdfs","text":"","code":"aggregate_cdfs(   data,   id_var,   group_by = NULL,   method,   ret_quantiles,   ret_values = NA,   weighting_scheme = \"equal\",   reorder_quantiles = TRUE,   ... )"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/aggregate_cdfs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"aggregate a set of cdfs — aggregate_cdfs","text":"data data.frame contains multiple cdfs, grouped column. Specify cdf quantile value columns, id_var string containing name column identifies unique cdfs group_by vector containing names columns create unique aggregates method character name method aggregation (see details). ret_quantiles vector quantiles return specifying aggregate distribution ret_values vector values return specifying aggregate distribution weighting_scheme string indicate weight aggregate (see details). reorder_quantiles TRUE ensure quantiles ordered; set FALSE quantiles data already ordered (increasing) ... additional arguments","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/aggregate_cdfs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"aggregate a set of cdfs — aggregate_cdfs","text":"TBD","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/aggregate_cdfs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"aggregate a set of cdfs — aggregate_cdfs","text":"Methods include: \"LOP\" - simple probability averaging, also called Linear Opinion Pool. \"vincent\" - simple quantile averaging, also called Vincent average. Weighting schemes include: \"equal\" - equal weighting models values \"user_defined\" - user supplies weights applied model (additional input weights data.frame containing id weight columns) trimming - \"cdf_interior\", \"cdf_exterior\", \"mean_interior\", \"mean_exterior\", following REF (additional inputs ...)","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/cdf_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Implement cdf trimming - TO UPDATE — cdf_trim","title":"Implement cdf trimming - TO UPDATE — cdf_trim","text":"Cdf trimming removes set number innermost/outermost probabilities. value averaging ADD REF.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/cdf_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Implement cdf trimming - TO UPDATE — cdf_trim","text":"","code":"cdf_trim(data, trim_type, n_trim, avg_dir)"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/cdf_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Implement cdf trimming - TO UPDATE — cdf_trim","text":"data data.frame containing columns id, quantile, value trim_type \"interior\" omitting central distributions \"exterior\" omitting outermost distributions n_trim integer, number ids trim avg_dir string specifying whether averaging \"vincent\" \"LOP\"","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/cdf_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Implement cdf trimming - TO UPDATE — cdf_trim","text":"data.frame containing original data argument plus additional column weight, specifies values excluded (.e., given 0 weight)","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/cdf_trim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Implement cdf trimming - TO UPDATE — cdf_trim","text":"keep_vals contain integer values 1 n_id, number unique ids. , value, trim_cdf() order probabilities keep defined keep_vals. example, n_id = 5 keep_vals = 2:4, trim_cdf() return second, third fourth ranked probability, excluding smallest larges probabilities average.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify values to trim — keep_vals","title":"Identify values to trim — keep_vals","text":"Based trimming direction, method, number, return values trim. : Lower/upper","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify values to trim — keep_vals","text":"","code":"keep_vals(trim_type, n_trim, n_ids)"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identify values to trim — keep_vals","text":"trim_type \"interior\" omitting central distributions \"exterior\" omitting outermost distributions n_trim integer, number ids trim n_ids integer, number unique ids aggregated","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Identify values to trim — keep_vals","text":"list containing two elements: (1) vector values keep; (2) number ids trimmed","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_exterior.html","id":null,"dir":"Reference","previous_headings":"","what":"Setup exterior trimming — keep_vals_exterior","title":"Setup exterior trimming — keep_vals_exterior","text":"exterior trimming, remove n_trim/2 lowest highest values.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_exterior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setup exterior trimming — keep_vals_exterior","text":"","code":"keep_vals_exterior(n_trim, n_ids)"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_exterior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Setup exterior trimming — keep_vals_exterior","text":"n_trim integer, number ids trim n_ids integer, number unique ids aggregated","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_exterior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setup exterior trimming — keep_vals_exterior","text":"list containing two elements: (1) vector values keep; (2) number ids trimmed","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_exterior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Setup exterior trimming — keep_vals_exterior","text":"","code":"if (FALSE) { keep_vals_exterior(2, 4) keep_vals_exterior(4, 5) }"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_interior.html","id":null,"dir":"Reference","previous_headings":"","what":"Setup interior trimming — keep_vals_interior","title":"Setup interior trimming — keep_vals_interior","text":"interior trimming, remove innermost n_trim values.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_interior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setup interior trimming — keep_vals_interior","text":"","code":"keep_vals_interior(n_trim, n_ids)"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_interior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Setup interior trimming — keep_vals_interior","text":"n_trim integer, number ids trim n_ids integer, number unique ids aggregated","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_interior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setup interior trimming — keep_vals_interior","text":"list containing two elements: (1) vector values keep; (2) number ids trimmed","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_interior.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Setup interior trimming — keep_vals_interior","text":"Interior trimming requires (1) n_trim < n_ids (2) n_trim n_ids parity (.e., even odd). , n_trim increased n_trim + 1.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/keep_vals_interior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Setup interior trimming — keep_vals_interior","text":"","code":"if (FALSE) { keep_vals_interior(2, 4) keep_vals_interior(1, 4) }"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/mean_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Implement mean trimming - TODO: UPDATE — mean_trim","title":"Implement mean trimming - TODO: UPDATE — mean_trim","text":"Trim entire distribution based mean value ADD REF.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/mean_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Implement mean trimming - TODO: UPDATE — mean_trim","text":"","code":"mean_trim(data, trim_type, n_trim)"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/mean_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Implement mean trimming - TODO: UPDATE — mean_trim","text":"data data.frame containing columns id, quantile, value trim_type \"interior\" omitting central distributions \"exterior\" omitting outermost distributions n_trim integer, number ids trim","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/mean_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Implement mean trimming - TODO: UPDATE — mean_trim","text":"data.frame containing original data argument plus additional column weight, specifies cdfs excluded (.e., given 0 weight)","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/tuesdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Weather Forecast Accuracy — tuesdata","title":"Weather Forecast Accuracy — tuesdata","text":"Weather Forecast Accuracy","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/tuesdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weather Forecast Accuracy — tuesdata","text":"","code":"tuesdata"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/tuesdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Weather Forecast Accuracy — tuesdata","text":"tt_data object 3 elements weather_forecasts data frame 651968 rows 10 variables cities data frame 236 rows 11 variables outlook_meanings data frame 23 rows 2 variables","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/tuesdata.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Weather Forecast Accuracy — tuesdata","text":"https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-12-20","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/vincent.html","id":null,"dir":"Reference","previous_headings":"","what":"Implement quantile averaging — vincent","title":"Implement quantile averaging — vincent","text":"Given set cumulative distribution functions  (assume now: defined values), combine using quantile averaging (also called Vincent average). method calculates (weighted) average values given quantile","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/vincent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Implement quantile averaging — vincent","text":"","code":"vincent(   quantile,   value,   id,   ret_quantiles,   ret_values,   weight_fn = equal_weights,   ... )"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/vincent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Implement quantile averaging — vincent","text":"quantile vector containing quantiles cdfs aggregated value vector containing values cdfs aggregated id vector containing unique ids distinguish cdf aggregated ret_quantiles vector quantiles return specifying aggregate distribution ret_values vector values return specifying aggregate distribution weight_fn function? specifying weight model *FIX ... additional arguments","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/vincent.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Implement quantile averaging — vincent","text":"vector values corresponding ret_quantiles aggregate distribution","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/reference/vincent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Implement quantile averaging — vincent","text":"","code":"dat <- expand.grid(id = c(\"A\", \"B\"),                    quantile = seq(0,1,0.01)) dat$value <- ifelse(dat$id == \"A\", qnorm(dat$quantile), qnorm(dat$quantile, 0,2)) vincent(dat$quantile, dat$value, dat$id, seq(0,1,0.05), NA) #> # A tibble: 21 × 2 #>    quantile    value #>       <dbl>    <dbl> #>  1     0    -Inf     #>  2     0.05   -2.47  #>  3     0.1    -1.92  #>  4     0.15   -1.55  #>  5     0.2    -1.26  #>  6     0.25   -1.01  #>  7     0.3    -0.787 #>  8     0.35   -0.578 #>  9     0.4    -0.380 #> 10     0.45   -0.188 #> # ℹ 11 more rows"},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/news/index.html","id":"combinedistributions-020","dir":"Changelog","previous_headings":"","what":"CombineDistributions 0.2.0","title":"CombineDistributions 0.2.0","text":"Added additional section SIRS-vignette illustrate exterior trimming (case study outlier prediction)","code":""},{"path":"https://jhu-statprogramming-fall-2023.github.io/biostat777-project3-part1-m-qin/news/index.html","id":"combinedistributions-010","dir":"Changelog","previous_headings":"","what":"CombineDistributions 0.1.0","title":"CombineDistributions 0.1.0","text":"Added NEWS.md file track changes package. Added ret_values argument allows user specify values aggregate CDF (return quantiles)","code":""}]
